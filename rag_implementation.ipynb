{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook builds a RAG system from scratch using python and FAISS vector store. I use sample pdf as the document to vectorise and retreive from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps to follow : \n",
    "    1. Open a document \n",
    "    2. format the text for the embedding model\n",
    "    3. embed all the chunks which can be stored for later\n",
    "    4. build a retrieval system that searches the vector store and returns the similar embeddings to teh query\n",
    "    5. create a prompt that incorporates the returned embeddings\n",
    "    6. generate an answer to the query based on the passages from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps 1-3 : Document preprocessing and embedding creation\n",
    "- Steps 4-6 : Search and Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document pre-processing and embedding creation\n",
    "\n",
    "Ingredients : \n",
    "- Data documents of any choice\n",
    "- embedding model of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "\n",
    "pdf_path = './Rag-From-Scratch/simple-local-rag/human-nutrition-text.pdf'\n",
    "\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print('File does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz \n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def text_formatter(strng):\n",
    "    cleaned_text = strng.replace('\\n', ' ').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\n",
    "            'page_number' : page_number - 41,\n",
    "            'page_char_count' : len(text),\n",
    "            'page_word_count' : len(text.split(' ')) if len(text) > 0 else 0,\n",
    "            'page_sentence_count' : len(text.split('. ')) if len(text) > 0 else 0,\n",
    "            'page_token_count' : len(text)/4, \n",
    "            'text' : text\n",
    "        })\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_text = open_and_read(pdf_path)\n",
    "len(pages_and_text)\n",
    "pages_and_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why care about the token count ?\n",
    "\n",
    "Token count is important because of the context window of the embedding model and the context window of the LLMs. By context window I mean, the maximum length of the input text provided to the embbedding model or the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# create a document instance as an example.\n",
    "doc = nlp('This a sentence. This is another sentence. This is the third sentence.')\n",
    "\n",
    "assert len(list(doc.sents)) == 3\n",
    "\n",
    "# print sentences split\n",
    "list(doc.sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our pdf dictionary\n",
    "pages_and_text[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(pages_and_text):\n",
    "    item['sentences'] = list(nlp(item['text']).sents)\n",
    "\n",
    "    # make sure all sentences are string. Default is a spacy datatype.\n",
    "    item['sentences'] = [str(strng) for strng in item['sentences']]\n",
    "\n",
    "    item['sentences_per_page'] = len(item['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages_and_text, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataframe\n",
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking approach\n",
    "\n",
    "- The concept of splitting larger pieces of text into smaller text of suitable sizes or chunking is done to provide appropriate sized inputs to the embedding model and LLM. \n",
    "- There is no, one correct way to chunk. It depends on the your use case. Some of the approaches to chunking are fixed size chunking, token and word based chunking, recursive token and word based chunking, semantic chunking, etc. \n",
    "- We will use fixed sized chunking here, and go with 10 sentences in a chunk.\n",
    "- Each page will be subdivided into chunks of 10 sentences or smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# create a function to split lists of sentences into chunk size, recursively\n",
    "def split_list(input_list):\n",
    "    slice_size = num_sentence_chunk_size\n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# test_list = list(range(25))\n",
    "# split_list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pages and split text into chunks\n",
    "\n",
    "for item in tqdm(pages_and_text):\n",
    "    item['sentence_chunks'] = split_list(item['sentences'])\n",
    "    item['num_chunks'] = len(item['sentence_chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_text, k =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_text)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting each chunk into its own item in the document dictionary. This gives a greater level of granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_text):\n",
    "    for sentence_chunk in item['sentence_chunks']:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict['page_number'] = item['page_number']\n",
    "        joined_sentence_chunk = ''.join(sentence_chunk).replace('  ', ' ').strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
    "\n",
    "        chunk_dict['sentence_chunk'] = joined_sentence_chunk\n",
    "        chunk_dict['chunk_char_count'] = len(joined_sentence_chunk)\n",
    "        chunk_dict['chunk_word_count'] = len(joined_sentence_chunk.split(' '))\n",
    "        chunk_dict['chunk_token_count'] = len(joined_sentence_chunk)/4\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter chunks with very small text length. These chunks might not have useful information.\n",
    "min_token_length = 30\n",
    "\n",
    "# for row in df[df['chunk_token_count']<min_token_length].sample(5).iterrows():\n",
    "#     print(f'Chunk token count : {row[1]['chunk_token_count']} | Text : {row[1]['sentence_chunk']}')\n",
    "\n",
    "pages_and_chunks_over_min_token_len = df[df['chunk_token_count'] > min_token_length].to_dict(orient='records')\n",
    "pages_and_chunks_over_min_token_len[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_len, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding our text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using an embedding model from sentence transformer library.\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "embedding_model = SentenceTransformer(model_name_or_path = 'all-mpnet-base-v2', device=device)\n",
    "\n",
    "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
    "#     # sentences are encoded by calling .encode on the model\n",
    "#     item['embeddings'] = embedding_model.encode(item['sentence_chunk'])\n",
    "\n",
    "text_chunks = [item['sentence_chunk'] for item in pages_and_chunks_over_min_token_len]\n",
    "# text_chunks[419]\n",
    "\n",
    "text_chunk_embeddings = embedding_model.encode(\n",
    "    text_chunks,\n",
    "    batch_size = 32,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "text_chunk_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing FAISS Vector store\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "# Convert the tensor from GPU to CPU and detach it from the graph\n",
    "# Then convert to a numpy array of type float32\n",
    "text_chunk_embeddings = np.array(text_chunk_embeddings.cpu(), dtype=np.float32)\n",
    "\n",
    "d = 768\n",
    "# setting up the vector store:\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = np.random.random((10, d)).astype('float32') # create random query\n",
    "\n",
    "k=4 #nearest 4 neighbours\n",
    "\n",
    "D,I = index.search(xq, k) #return distances and indices for each query\n",
    "print(I)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Retreive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a re-rank model\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranking_model = CrossEncoder('mixedbread-ai/mxbai-rerank-large-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert query to embeddings using the same embedding model used to embed the data documents.\n",
    "\n",
    "query = 'macronutrients functions'\n",
    "\n",
    "# embed the query\n",
    "query_embed = embedding_model.encode(query, convert_to_tensor=True)\n",
    "query_embed = query_embed.cpu().reshape(1,-1)\n",
    "\n",
    "D,I = index.search(query_embed, k)\n",
    "\n",
    "\n",
    "print(f'I : {I}')\n",
    "print(f'D : {D}')\n",
    "\n",
    "for dist, idx in zip(D[0], I[0]):\n",
    "    print(f'Distance : {dist}')\n",
    "    print(f'Text : {pages_and_chunks_over_min_token_len[idx]['sentence_chunk']}')\n",
    "    print(f'Page number : {pages_and_chunks_over_min_token_len[idx]['page_number']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could potentially improve the results by using a re-ranking model. The model is trained specifically to re-rank the search results and rank them in the order most likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_docs = [pages_and_chunks_over_min_token_len[idx]['sentence_chunk'] for idx in I[0]]\n",
    "\n",
    "results = reranking_model.rank(query, retreived_docs, return_documents=True, top_k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query, model, num_res_to_return):\n",
    "    '''\n",
    "    Embeds a query with the used model and returns top k scores and indices from vector store.\n",
    "    '''\n",
    "\n",
    "    # embed the query\n",
    "    query_embed = model.encode(query, convert_to_tensor=True)\n",
    "    query_embed = query_embed.cpu().reshape(1,-1)\n",
    "    D,I = index.search(query_embed, num_res_to_return+5)\n",
    "    retreived_docs = [pages_and_chunks_over_min_token_len[idx]['sentence_chunk'] for idx in I[0]]\n",
    "    \n",
    "    return reranking_model.rank(query, retreived_docs, return_documents=True, top_k=num_res_to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id \n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                #  quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "# if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "llm_model.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_num_params(model):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mem_size(model):\n",
    "    # get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # calculate model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2)\n",
    "    model_mem_gb = model_mem_bytes / (1024**3)\n",
    "\n",
    "    return {\n",
    "        'model_mem_bytes' : model_mem_bytes,\n",
    "        'model_mem_mb' : round(model_mem_mb, 2),\n",
    "        'model_mem_gb' : round(model_mem_gb, 2)\n",
    "    }\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of questions\n",
    "query_list = [\n",
    "    'How can I tell if Iâ€™m getting enough micronutrients from fruits and vegetables?',\n",
    "    'How does hydration influence overall energy levels and metabolism?',\n",
    "    'What are some effective strategies for increasing daily fiber intake?',\n",
    "    'What differences exist between plant-based proteins and animal-based proteins in terms of amino acid profiles?',\n",
    "    'What is the relationship between gut health, the microbiome, and nutrient absorption?',\n",
    "    'How often should infants be breastfed?',\n",
    "    'What are the symptoms of pellagra?',\n",
    "    'How does saliva help with digestion?',\n",
    "    'What is the RDI for protein per day?',\n",
    "    'Water soluble vitamins',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "query = random.choice(query_list)\n",
    "print(f'Query : {query}')\n",
    "prompt = query\n",
    "# get the scores and indices from RAG\n",
    "context_list = retrieve_relevant_resources(prompt, embedding_model, 5)\n",
    "\n",
    "print(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment our prompt with the context items:\n",
    "\n",
    "# Prompting techniques to use : \n",
    "'''\n",
    "1.Give clear intructions.\n",
    "2.Give a few input/output examples:(Manual COT)\n",
    "3.Ask to work the query, step by step:(Automatic COT), Give step by step reasoning.\n",
    "'''\n",
    "\n",
    "def prompt_formatter(query, context_items):\n",
    "    context = '-'+'\\n-'.join([item['text'] for item in context_items])\n",
    "    base_prompt = base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "    prompt = base_prompt.format(context = context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_formatter(prompt, context_list)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat template\n",
    "chat = [{\n",
    "    'role' : 'user',\n",
    "    'content' : prompt\n",
    "}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize the input text\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate output from the local LLM\n",
    "outputs = llm_model.generate(**input_ids, \n",
    "                             temperature=0.7,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=256)\n",
    "outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "outputs_decoded = outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')\n",
    "outputs_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(prompt,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=256,\n",
    "        format_answer_only=True,\n",
    "        return_answer_only=True):\n",
    "    '''\n",
    "    Takes the query, finds the relevant resources and generates the answer to the query based on the relevant resources from the private documents.\n",
    "    '''\n",
    "\n",
    "    # get the scores and indices from RAG\n",
    "    context_list = retrieve_relevant_resources(prompt, embedding_model, 5)\n",
    "    prompt = prompt_formatter(prompt, context_list)\n",
    "\n",
    "    # Create a chat template\n",
    "    chat = [{\n",
    "        'role' : 'user',\n",
    "        'content' : prompt\n",
    "        }]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # tokenize the input text\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate output from the local LLM\n",
    "    outputs = llm_model.generate(**input_ids, \n",
    "                             temperature=temperature,\n",
    "                             do_sample=True,\n",
    "                             max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    outputs_decoded = tokenizer.decode(outputs[0])\n",
    "    if format_answer_only:\n",
    "        outputs_decoded = outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')\n",
    "    return outputs_decoded, context_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask('What are the fat-soluble vitamins?', temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azmat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
